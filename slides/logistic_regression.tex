\documentclass[12pt]{article}

\usepackage{amsmath,amsthm,amssymb,hyperref,fullpage,palatino,bm}

\newtheorem{thm}{Theorem}
\newtheorem{cor}{Corollary}
\newtheorem{lem}{Lemma}
\newtheorem{con}[thm]{Conjecture}
\newtheorem{rem}{Remark}
\newtheorem{dfn}{Definition}

\newcommand{\R}{\mathbb{R}}

\newcommand{\x}{\boldsymbol{x}}
\newcommand{\X}{\boldsymbol{X}}
\newcommand{\y}{\boldsymbol{y}}
\newcommand{\yh}{\hat{\boldsymbol{y}}}
\newcommand{\w}{\boldsymbol{w}}
\newcommand{\A}{\boldsymbol{A}}

\newcommand{\mse}{\mathrm{MSE}}

\title{Gradients of logistic regression with squared error and binary cross-entropy loss functions}
\author{Pawel Wocjan}

\begin{document}

\maketitle


\begin{abstract}
We derive the gradients for logistic regression with (a) mean squared error and (b) binary cross-entropy as loss functions. You will need these results for the first homework.
\end{abstract}

\section{Logistic regression}

Let 
\begin{equation}
\w=
\left(
\begin{array}{c}
w_1 \\
w_2 \\
\vdots \\
w_n
\end{array}
\right)\in\R^n
\quad
\mbox{and}
\quad
b \in \R
\end{equation}
be the weight vector and the bias of a neuron, respectively.  Let $a : \R \rightarrow \R$ denote its activation function $a$. The neuron outputs
\begin{equation}
\hat{y} = a\left( \sum_{j=1}^n w_j x_j + b \right)
\end{equation}
when given a feature vector $\x=(x_1,\ldots,x_n)^T\in\R^n$ as input.
We use $z$ to denote
\begin{equation}
z = \sum_{j=1}^n w_j x_j + b\,.
\end{equation}
The function implemented by the neuron consists of two steps:
\begin{equation}
\x \quad \mapsto \quad z = \w^T \x + b = \sum_{j=1}^n w_j x_j + b \quad \mapsto \quad a = a(z).
\end{equation}
For logistic regression the activation function $a(z)$ is equal to the sigmoid function $\sigma(z)$, which is defined by 
\begin{equation}
\sigma(z) = \frac{1}{1+e^{-z}}.
\end{equation}
The sigmoid function $\sigma$ maps $\R$ to the open interval $(0,1)$. It satisfies the following properties:
\begin{eqnarray}
\sigma(0) & = & \frac{1}{2} \\
\sigma(-z) & = & 1 - \sigma(z) \\
\lim_{z\rightarrow -\infty} & = & 0 \\
\lim_{z\rightarrow \infty} & = & 1.
\end{eqnarray}
Its derivative $\sigma'(z)$ is obtained by applying the chain rule and simple algebraic manipulations:
\begin{eqnarray}
\sigma'(z) 
& = & 
-\frac{1}{(1+e^{-z})^2} \cdot e^{-z} \cdot (-1) \\
& = &
\frac{1}{1+e^{-z}} \cdot \frac{e^{-z}}{1+e^{-z}} \\
& = &
\frac{1}{1+e^{-z}} \cdot \frac{1 + e^{-z} - 1}{1+e^{-z}} \\
& = &
\sigma(z) \cdot (1-\sigma(z)) 
\end{eqnarray}

Logistic regression can be used for binary classification, which is the task of classifying the feature vectors into two classes, denoted by $0$ and $1$.
The activation $\sigma(z)$ can be interpreted as the probability that the neuron assigns to the class $1$. Using this probability, we make a prediction as follows:
\begin{eqnarray}
\mbox{class $0$} & \mbox{if} & \sigma(z) < \frac{1}{2} \\
\mbox{class $1$} & \mbox{if} & \sigma(z) \ge \frac{1}{2}
\end{eqnarray}

\section{Squared error and binary entropy loss functions}

We consider two loss functions for logistic regression: squared error and binary cross-entropy. 
Let $\x\in\R^n$ be a feature vector and $y\in\{0,1\}$ its correct label.

\begin{itemize}
\item The squared error loss $\mathcal{L}_\mathrm{se}$ is defined by
\begin{equation}
\mathcal{L}_\mathrm{se} = \frac{1}{2} (a - y)^2\,.
\end{equation}
Its derivative with respect to $a$ is equal to 
\begin{equation}
{\mathrm{d} \mathcal{L}_\mathrm{se} \over \mathrm{d} a} = a - y.
\end{equation}
%
%
%
\item The (binary) cross-entropy loss is defined by
\begin{equation}
\mathcal{L}_\mathrm{ce} = - y \log a - (1-y) \log(1-a)\,.
\end{equation}
Its derivative with respect with $a$ is equal to 
\begin{equation}
{\mathrm{d} \mathcal{L}_\mathrm{be} \over \mathrm{d} a} = -\frac{y}{a} + \frac{1-y}{1-a}.
\end{equation}
\end{itemize}

I will explain the intuition behind the cross-entropy loss in class in detail. For the analysis, consider the following two cases:
\begin{itemize}
\item If the true label is $y=1$, then the loss is equal to $-\log a$, which is a strictly decreasing function on the open interval $(0,1)$.  The loss tends to $\infty$ as $a\rightarrow 0$ and to $0$ as $a\rightarrow 1$, respectively.
\item If the true label is $y=0$, then the loss is equal to $-\log(1-a)$, which is a strictly increasing function of the open interval $(0,1)$.  The loss tends to $\infty$ as $a\rightarrow 1$ and to $0$ as $a\rightarrow 0$, respectively.
\end{itemize}

\section{Gradient of squared error and binary cross-entropy loss functions}

We have to compute the partial derivatives of the loss functions with respect to $w_j$ and $b$ to be able to apply stochastic gradient descent. This is done by applying the chain rule multiple times according to the following computational graph: 
\begin{equation}
w_1,\ldots,w_n, b \rightarrow z \rightarrow a \rightarrow \mathcal{L},
\end{equation}
where $\rightarrow$ indicates that the variables on the LHS influence those on the RHS.

Recall that derivative of the activation function $a$ is $a' = (1-a)$ because the sigmoid function is used as the activation function for logistic regression.
\begin{itemize}
\item 
The partial derivatives of the squared error loss $\mathcal{L}_\mathrm{se}$ are derived as follows:
\begin{align}
{\partial \mathcal{L}_\mathrm{se} \over \partial w_j}
&=  
{\mathrm{d} \mathcal{L}_\mathrm{se} \over \mathrm{d}a} \cdot {\partial a \over \partial z} \cdot {\partial z \over \partial w_j} =
(a - y) \cdot a' \cdot x_j  \\
& \notag \\
%
{\partial \mathcal{L}_\mathrm{se} \over \partial b} 
&=
{\partial \mathcal{L}_\mathrm{se} \over \partial b} \cdot  {\partial a \over \partial z} \cdot {\partial z \over \partial b} =
(a - y) \cdot a'
\end{align}
Note that $a'\approx 0$ whenever $z$ is either very small or very large. In either case, we say that the neuron is saturated. The problem is that a saturated neuron learns slowly when $a$ is very far from $y$. This can be improved by using the binary cross entropy loss function.
\item
The partial derivatives of the cross entropy loss $\mathcal{L}_\mathrm{ce}$ are derived as follows:
\begin{align}
{\partial \mathcal{L}_\mathrm{ce} \over \partial w_j}
&=  
{\mathrm{d} \mathcal{L}_\mathrm{ce} \over \mathrm{d}a} \cdot {\partial a \over \partial z} \cdot {\partial z \over \partial w_j} \\
&= 
\left(-\frac{y}{a} + \frac{1-y}{1-a} \right) \cdot a' \cdot x_j  \\
&=
\left(-\frac{y}{a} + \frac{1-y}{1-a} \right) \cdot a \cdot (1 - a) \cdot x_j  \\
&=
\Big( - y \cdot  (1 - a) + (1-y) \cdot a \Big) \cdot x_j \\
&=
(a - y) \cdot x_j \\
%
& \notag \\
{\partial \mathcal{L}_\mathrm{ce} \over \partial b} 
&=
{\mathrm{d} \mathcal{L}_\mathrm{ce} \over \mathrm{d}a} \cdot {\partial a \over \partial z} \cdot {\partial z \over \partial b} = 
a - y.
\end{align}
Here it is essential that we expand $a'$ as $a \cdot (1-a)$ to cancel out the denominators $a$ and $1-a$.
\end{itemize}

\end{document}